{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chapter 2: Working with Text Data\n",
   "id": "bb9a4177512e548f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "notebook based on chapter 2 from the book",
   "id": "4cac9f26e88a9255"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-24T15:50:32.393307Z",
     "start_time": "2025-12-24T15:50:32.385910Z"
    }
   },
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.8.0+cu126\n",
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/01.webp?timestamp=1\" width=\"500px\">\n",
   "id": "c045518c8afe90bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.2 Tokenizing text\n",
    "\n",
    "Core idea: convert words to token than can later be transformed into embeddings"
   ],
   "id": "d3ae1b21fa958081"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/04.webp\" width=\"300px\">\n",
   "id": "2c79bce071fcb4d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:25:30.872554Z",
     "start_time": "2025-12-24T15:25:30.868385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Download data\n",
    "\n",
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ],
   "id": "6ba7907251594818",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:25:30.882649Z",
     "start_time": "2025-12-24T15:25:30.872554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ],
   "id": "38a28a58c0d59835",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:25:30.888815Z",
     "start_time": "2025-12-24T15:25:30.884974Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # first we split the text into words\n",
    "\n",
    "import re\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "\n",
    "print(result)"
   ],
   "id": "a107bbfcc2868a5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:25:30.895526Z",
     "start_time": "2025-12-24T15:25:30.891611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# also split with commas and signs, we modify the regular expression\n",
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "print(result)"
   ],
   "id": "376fadc14c6c488d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:25:30.901192Z",
     "start_time": "2025-12-24T15:25:30.895526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# delete spaces in words, and remove empty strings.\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ],
   "id": "39fb7e949507375",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:25:30.908745Z",
     "start_time": "2025-12-24T15:25:30.901192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# split the complete text file\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])\n",
    "print(f\"number of 'tokens' in the text: {len(preprocessed)}\")"
   ],
   "id": "bf418090d42b50b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n",
      "number of 'tokens' in the text: 4690\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.3 Converting tokens into token IDs\n",
    "\n",
    "Core idea: convert tokes into ids to use in embeddings layer\n"
   ],
   "id": "6f5cd52f9049acb0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/06.webp\" width=\"500px\">",
   "id": "f385ed48e2d78fab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:25:30.915406Z",
     "start_time": "2025-12-24T15:25:30.909194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# first we remove duplicated tokens/words\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ],
   "id": "c1406b48470b6c8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:25:30.921424Z",
     "start_time": "2025-12-24T15:25:30.918010Z"
    }
   },
   "cell_type": "code",
   "source": "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
   "id": "c598b6ca73bee156",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:25:30.928637Z",
     "start_time": "2025-12-24T15:25:30.924801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "first_twenty = list(vocab.items())[:20]\n",
    "print(first_twenty)"
   ],
   "id": "5c85107c04ada983",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('!', 0), ('\"', 1), (\"'\", 2), ('(', 3), (')', 4), (',', 5), ('--', 6), ('.', 7), (':', 8), (';', 9), ('?', 10), ('A', 11), ('Ah', 12), ('Among', 13), ('And', 14), ('Are', 15), ('Arrt', 16), ('As', 17), ('At', 18), ('Be', 19)]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/07.webp?123\" width=\"500px\">",
   "id": "43e7eb09e7968220"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:25:30.935807Z",
     "start_time": "2025-12-24T15:25:30.928637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        # from text to token ids\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) # split text\n",
    "\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ] # remove spaces\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # from token ids back to text\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ],
   "id": "ab14fb0c72f85115",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:25:30.942448Z",
     "start_time": "2025-12-24T15:25:30.938515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# remember that vocab is a dictionary with token: id\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "\n",
    "# for now, we cant have a word that it is not in the vocab\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ],
   "id": "9b37d0b9cfff0fc2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:25:30.952609Z",
     "start_time": "2025-12-24T15:25:30.942886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(tokenizer.decode(ids))\n",
    "tokenizer.decode(tokenizer.encode(text))\n",
    "\n"
   ],
   "id": "a813b38b20e620d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.4 Adding special context tokens\n",
    "\n",
    "\n",
    "Core idea: it is useful to have some special tokens that indicate something in the text like the end of the text\n"
   ],
   "id": "c4b7a31ec572bb43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Some tokenizers use special tokens to help the LLM with additional context\n",
    "- Some of these special tokens are\n",
    "  - `[BOS]` (beginning of sequence) marks the beginning of text\n",
    "  - `[EOS]` (end of sequence) marks where the text ends (this is usually used to concatenate multiple unrelated texts, e.g., two different Wikipedia articles or two different books, and so on)\n",
    "  - `[PAD]` (padding) if we train LLMs with a batch size greater than 1 (we may include multiple texts with different lengths; with the padding token we pad the shorter texts to the longest length so that all texts have an equal length)\n",
    "- `[UNK]` to represent words that are not included in the vocabulary\n",
    "\n",
    "- Note that GPT-2 does not need any of these tokens mentioned above but only uses an `<|endoftext|>` token to reduce complexity\n",
    "- The `<|endoftext|>` is analogous to the `[EOS]` token mentioned above\n",
    "- GPT also uses the `<|endoftext|>` for padding (since we typically use a mask when training on batched inputs, we would not attend padded tokens anyway, so it does not matter what these tokens are)\n",
    "- GPT-2 does not use an `<UNK>` token for out-of-vocabulary words; instead, GPT-2 uses a byte-pair encoding (BPE) tokenizer, which breaks down words into subword units which we will discuss in a later section\n",
    "- We use the `<|endoftext|>` tokens between two independent sources of text"
   ],
   "id": "baa152b8feb39365"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:25:31.437656Z",
     "start_time": "2025-12-24T15:25:30.952609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# let´s see what happens when we use words not in the vocab\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"Hello, do you like tea. Is this-- a test?\"\n",
    "\n",
    "tokenizer.encode(text)"
   ],
   "id": "688e14942e8769d3",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 6\u001B[0m\n\u001B[0;32m      2\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m SimpleTokenizerV1(vocab)\n\u001B[0;32m      4\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHello, do you like tea. Is this-- a test?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 6\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39mencode(text)\n",
      "Cell \u001B[1;32mIn[11], line 14\u001B[0m, in \u001B[0;36mSimpleTokenizerV1.encode\u001B[1;34m(self, text)\u001B[0m\n\u001B[0;32m      8\u001B[0m preprocessed \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m([,.:;?_!\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m()\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124m]|--|\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms)\u001B[39m\u001B[38;5;124m'\u001B[39m, text) \u001B[38;5;66;03m# split text\u001B[39;00m\n\u001B[0;32m     10\u001B[0m preprocessed \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     11\u001B[0m     item\u001B[38;5;241m.\u001B[39mstrip() \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m preprocessed \u001B[38;5;28;01mif\u001B[39;00m item\u001B[38;5;241m.\u001B[39mstrip()\n\u001B[0;32m     12\u001B[0m ] \u001B[38;5;66;03m# remove spaces\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m ids \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstr_to_int[s] \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m preprocessed]\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ids\n",
      "Cell \u001B[1;32mIn[11], line 14\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      8\u001B[0m preprocessed \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m([,.:;?_!\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m()\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124m]|--|\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms)\u001B[39m\u001B[38;5;124m'\u001B[39m, text) \u001B[38;5;66;03m# split text\u001B[39;00m\n\u001B[0;32m     10\u001B[0m preprocessed \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     11\u001B[0m     item\u001B[38;5;241m.\u001B[39mstrip() \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m preprocessed \u001B[38;5;28;01mif\u001B[39;00m item\u001B[38;5;241m.\u001B[39mstrip()\n\u001B[0;32m     12\u001B[0m ] \u001B[38;5;66;03m# remove spaces\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m ids \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstr_to_int[s] \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m preprocessed]\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ids\n",
      "\u001B[1;31mKeyError\u001B[0m: 'Hello'"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:25:59.184185Z",
     "start_time": "2025-12-24T15:25:59.179424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# we extend the vocab with these special tokens\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "\n",
    "list(vocab.items())[-5:]"
   ],
   "id": "faaa37f2c87272c4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('younger', 1127),\n",
       " ('your', 1128),\n",
       " ('yourself', 1129),\n",
       " ('<|endoftext|>', 1130),\n",
       " ('<|unk|>', 1131)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:26:12.389055Z",
     "start_time": "2025-12-24T15:26:12.384916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ] # if item not in vocab add token <|unk|>\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ],
   "id": "c836ae8b6da592c6",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:26:13.080711Z",
     "start_time": "2025-12-24T15:26:13.077370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test tokenizer\n",
    "# with <|endoftext|> token remember that in vocab we have  ('<|endoftext|>', 1130),\n",
    "# some tokens not in vocab like \"Hello\"\n",
    "\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ],
   "id": "f6660dd688d240e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:26:14.022552Z",
     "start_time": "2025-12-24T15:26:14.018811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ],
   "id": "5d8017a1c4ad1015",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.5 BytePair encoding\n",
    "\n",
    "Core idea: use BytePairEncoding (BPE) as tokenizer, it allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words\n",
    "\n"
   ],
   "id": "b4177962068a0cbe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:29:27.036440Z",
     "start_time": "2025-12-24T15:29:26.998728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ],
   "id": "45e697c2fda18ea6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:29:40.146955Z",
     "start_time": "2025-12-24T15:29:34.734410Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
   "id": "db54a61933746082",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:32:11.539892Z",
     "start_time": "2025-12-24T15:32:11.536037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ],
   "id": "41371c595121956d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:32:12.278157Z",
     "start_time": "2025-12-24T15:32:12.274650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ],
   "id": "41834dfde5086723",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/11.webp\" width=\"300px\">\n",
   "id": "79756a4e706641f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.6 Data sampling with a sliding window\n",
    "\n",
    "Core idea: generate text input and the next word to predict in a specific format\n"
   ],
   "id": "be8dcdbe5f7918cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/12.webp\" width=\"400px\">",
   "id": "4efbd2b24c1f1068"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:32:47.236358Z",
     "start_time": "2025-12-24T15:32:47.228538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ],
   "id": "93735ef2f46bb6cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- For each text chunk, we want the inputs and targets\n",
    "- Since we want the model to predict the next word, the targets are the inputs shifted by one position to the right"
   ],
   "id": "cf262fcf6bce63b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:32:51.647362Z",
     "start_time": "2025-12-24T15:32:51.643353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# exampl\n",
    "enc_sample = enc_text[50:]\n",
    "context_size = 4 # how many tokes we use to predict the next one\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")\n"
   ],
   "id": "a0bf02386a905ca0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:32:56.781719Z",
     "start_time": "2025-12-24T15:32:56.777753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    ## input tokens ----> token to predict\n",
    "    print(context, \"---->\", desired)"
   ],
   "id": "c417cf184063d0ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:33:43.008889Z",
     "start_time": "2025-12-24T15:33:43.004854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ],
   "id": "da2a19a2043a1579",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For now, we implement a simple data loader that iterates over the input dataset and returns the inputs and targets shifted by one\n",
   "id": "95baf4fb1a4d5c83"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We use a sliding window approach, changing the position by +1:\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/13.webp?123\" width=\"500px\">\n"
   ],
   "id": "46de8dfb8b513179"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:37:43.656875Z",
     "start_time": "2025-12-24T15:37:40.062425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max_length+1\"\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ],
   "id": "67b15e893bca85d8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\envs\\LLMScratch\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:279: UserWarning: Failed to initialize NumPy: DLL load failed while importing _multiarray_umath: No se puede encontrar el módulo especificado. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:37:47.258511Z",
     "start_time": "2025-12-24T15:37:47.254312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ],
   "id": "4bd37461ff5448b8",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:37:53.379157Z",
     "start_time": "2025-12-24T15:37:53.374696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ],
   "id": "84b3084bdf009f19",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:38:11.598142Z",
     "start_time": "2025-12-24T15:38:11.509426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ],
   "id": "65fffc2186a42305",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:38:21.121118Z",
     "start_time": "2025-12-24T15:38:21.116407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ],
   "id": "c8d6f595824694c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "An example using stride equal to the context length (here: 4) as shown below:\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/14.webp\" width=\"500px\">\n"
   ],
   "id": "43435fd1a0ba833f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:43:00.403910Z",
     "start_time": "2025-12-24T15:43:00.340106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# we can also create batches for training\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=3, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ],
   "id": "4536f7bdb140348c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[  40,  367, 2885, 1464],\n",
      "        [ 367, 2885, 1464, 1807],\n",
      "        [2885, 1464, 1807, 3619]])\n",
      "\n",
      "Targets:\n",
      " tensor([[ 367, 2885, 1464, 1807],\n",
      "        [2885, 1464, 1807, 3619],\n",
      "        [1464, 1807, 3619,  402]])\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.7 Creating token embeddings\n",
    "\n",
    "Core idea: embed the tokens in a vector space using an embedding layer\n",
    "\n"
   ],
   "id": "b98a2a0d7194a30c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/15.webp\" width=\"400px\">\n",
    "m"
   ],
   "id": "b29e41d12e6c95a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:50:47.066044Z",
     "start_time": "2025-12-24T15:50:47.055025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## we have the following four input examples with input ids 2, 3, 5, and 1 after tokenization\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "\n",
    "# for the sake of simplicity, suppose we have a small vocabulary of only 6 words, and we want to create embeddings of size 3:\n",
    "\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim) # 6x3 matrix\n",
    "# so one_hot_encoding_word.T * embedding_layer has dimentions  (1 x vocab_size) * (vocab_size x3) = (1x3) wich is inthe output_dim"
   ],
   "id": "7ba62489966cf8fe",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T20:54:13.290629Z",
     "start_time": "2025-12-24T20:54:13.180418Z"
    }
   },
   "cell_type": "code",
   "source": "embedding_layer.weight",
   "id": "4dd539b8e6672a3a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3374, -0.1778, -0.1690],\n",
       "        [ 0.9178,  1.5810,  1.3010],\n",
       "        [ 1.2753, -0.2010, -0.1606],\n",
       "        [-0.4015,  0.9666, -1.1481],\n",
       "        [-1.1589,  0.3255, -0.6315],\n",
       "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T20:54:40.638679Z",
     "start_time": "2025-12-24T20:54:40.621409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(embedding_layer(torch.tensor([3])))\n",
    " # equivalent to token 4 transpoce x embedding_layer = [0,0,0,1,0,0] *  embedding_layer\n"
   ],
   "id": "a415adcf48c39d80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T20:57:41.429930Z",
     "start_time": "2025-12-24T20:57:41.423725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#input_ids = [2, 3, 5, 1]\n",
    "print(embedding_layer(input_ids)) # permutation of matrix rows: row 3,4,6,2\n"
   ],
   "id": "fc6602d8aee5649d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T20:56:08.967765Z",
     "start_time": "2025-12-24T20:56:08.958777Z"
    }
   },
   "cell_type": "markdown",
   "source": "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/16.webp?123\" width=\"500px\">\n",
   "id": "449fa837d8117dd0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.8 Encoding word positions\n",
    "\n",
    "Core idea: take into account the word position for the embedding\n"
   ],
   "id": "1d2d7490f42cdd3c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Embedding layer convert IDs into identical vector representations regardless of where they are located in the input sequence, positional embeddings are combined with the token embedding vector to form the input embeddings for a large language model\n",
   "id": "e17e2efe751049bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/18.webp\" width=\"500px\">,",
   "id": "f03de08ec16dd333"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T21:33:58.465932Z",
     "start_time": "2025-12-24T21:33:58.357807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The BytePair encoder has a vocabulary size of 50.257:\n",
    "# Suppose we want to encode the input tokens into a 256-dimensional vector representation:\n",
    "\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim) # 50257 * 256"
   ],
   "id": "5aac4615346a4dde",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T21:53:59.094241Z",
     "start_time": "2025-12-24T21:53:59.067246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# If we sample data from the dataloader, we embed the tokens in each batch into a 256-dimensional vector\n",
    "# If we have a batch size of 8 with 4 tokens each, this results in a 8 x 4 x 256 tensor:\n",
    "\n",
    "# stride: how much we slide our window per exameple\n",
    "# max_lenth: length of \"sentences\" during training, not to confuse with the trained model context\n",
    "\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ],
   "id": "21f344aa07d705ca",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T21:53:59.560278Z",
     "start_time": "2025-12-24T21:53:59.556277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ],
   "id": "3665377d52dc92da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T22:46:26.072809Z",
     "start_time": "2025-12-24T22:46:26.044770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)\n",
    "print(token_embeddings)"
   ],
   "id": "28fcfcc2211189de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n",
      "tensor([[[ 0.4913,  1.1239,  1.4588,  ..., -0.3995, -1.8735, -0.1445],\n",
      "         [ 0.4481,  0.2536, -0.2655,  ...,  0.4997, -1.1991, -1.1844],\n",
      "         [-0.2507, -0.0546,  0.6687,  ...,  0.9618,  2.3737, -0.0528],\n",
      "         [ 0.9457,  0.8657,  1.6191,  ..., -0.4544, -0.7460,  0.3483]],\n",
      "\n",
      "        [[ 1.5460,  1.7368, -0.7848,  ..., -0.1004,  0.8584, -0.3421],\n",
      "         [-1.8622, -0.1914, -0.3812,  ...,  1.1220, -0.3496,  0.6091],\n",
      "         [ 1.9847, -0.6483, -0.1415,  ..., -0.3841, -0.9355,  1.4478],\n",
      "         [ 0.9647,  1.2974, -1.6207,  ...,  1.1463,  1.5797,  0.3969]],\n",
      "\n",
      "        [[-0.7713,  0.6572,  0.1663,  ..., -0.8044,  0.0542,  0.7426],\n",
      "         [ 0.8046,  0.5047,  1.2922,  ...,  1.4648,  0.4097,  0.3205],\n",
      "         [ 0.0795, -1.7636,  0.5750,  ...,  2.1823,  1.8231, -0.3635],\n",
      "         [ 0.4267, -0.0647,  0.5686,  ..., -0.5209,  1.3065,  0.8473]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.6156,  0.9610, -2.6437,  ..., -0.9645,  1.0888,  1.6383],\n",
      "         [-0.3985, -0.9235, -1.3163,  ..., -1.1582, -1.1314,  0.9747],\n",
      "         [ 0.6089,  0.5329,  0.1980,  ..., -0.6333, -1.1023,  1.6292],\n",
      "         [ 0.3677, -0.1701, -1.3787,  ...,  0.7048,  0.5028, -0.0573]],\n",
      "\n",
      "        [[-0.1279,  0.6154,  1.7173,  ...,  0.3789, -0.4752,  1.5258],\n",
      "         [ 0.4861, -1.7105,  0.4416,  ...,  0.1475, -1.8394,  1.8755],\n",
      "         [-0.9573,  0.7007,  1.3579,  ...,  1.9378, -1.9052, -1.1816],\n",
      "         [ 0.2002, -0.7605, -1.5170,  ..., -0.0305, -0.3656, -0.1398]],\n",
      "\n",
      "        [[-0.9573,  0.7007,  1.3579,  ...,  1.9378, -1.9052, -1.1816],\n",
      "         [-0.0632, -0.6548, -1.0296,  ..., -0.9538, -0.5026, -0.1128],\n",
      "         [ 0.6032,  0.8983,  2.0722,  ...,  1.5242,  0.2030, -0.3002],\n",
      "         [ 1.1274, -0.1082, -0.2195,  ...,  0.5059, -1.8138, -0.0700]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T22:48:26.943588Z",
     "start_time": "2025-12-24T22:48:26.937339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "print(pos_embedding_layer.weight.shape)\n",
    "\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))"
   ],
   "id": "56ca82bffb5facc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T22:52:01.854333Z",
     "start_time": "2025-12-24T22:52:01.849457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# To create the input embeddings used in an LLM, we simply add the token and the positional embeddings\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ],
   "id": "e6fadc355c926feb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/19.webp\" width=\"400px\">",
   "id": "fc9faa5c6266e2af"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
